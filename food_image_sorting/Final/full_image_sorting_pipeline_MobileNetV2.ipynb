{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G-ZWy7Sv9JVU",
        "qb_UKuIV-Qsu",
        "cl2PJaZ4-dKW",
        "Kr7BWUiN-p0I"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Second Project - First Problem"
      ],
      "metadata": {
        "id": "ixcQEqiz8G7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Validation"
      ],
      "metadata": {
        "id": "Czvj5w0c-C7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the necessary Libraries"
      ],
      "metadata": {
        "id": "tpokcoM_7u2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import gdown"
      ],
      "metadata": {
        "id": "ea3dsXJ38MsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing some of the test data"
      ],
      "metadata": {
        "id": "A85TSqEp_Ckr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODo"
      ],
      "metadata": {
        "id": "H4zLmQON_He6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Settimg the File IDs and adjustments"
      ],
      "metadata": {
        "id": "ij5cC_St8M-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âš™ï¸ Settings\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "TRAIN_ZIP_ID = \"15CHt2ueS4c7emHpmzFHC3c0TGd51Mnvz\"\n",
        "TRAIN_ZIP_PATH = \"train_data.zip\"\n",
        "TRAIN_DIR = \"data/train\"\n",
        "TEST_ZIP_ID = \"1mtigz-kMPtI_IJKUmsG_wqNIhuBFUGdu\"\n",
        "TEST_ZIP_PATH = \"test_data.zip\"\n",
        "TEST_DIR = \"data/test\"\n",
        "MODEL_PATH = \"MobileNetV2_finetuned_final.h5\""
      ],
      "metadata": {
        "id": "TuU_seX08Zg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download, Extraction, Augmentation, Cleaning and Indexing the classes of Training Data"
      ],
      "metadata": {
        "id": "suUSpgDJ8bEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and Extraction\n",
        "if not os.path.exists(TRAIN_DIR):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={TRAIN_ZIP_ID}\", TRAIN_ZIP_PATH, quiet=False)\n",
        "    with zipfile.ZipFile(TRAIN_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data\")\n",
        "\n",
        "# Data Cleaning\n",
        "def clean_directory(directory):\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for fname in files:\n",
        "            fpath = os.path.join(root, fname)\n",
        "            try:\n",
        "                with Image.open(fpath) as img:\n",
        "                    img.verify()\n",
        "            except (UnidentifiedImageError, OSError):\n",
        "                os.remove(fpath)\n",
        "\n",
        "clean_directory(TRAIN_DIR)\n",
        "\n",
        "# Data Augmentation\n",
        "train_gen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Train and Validation seperation\n",
        "train_data = train_gen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', subset='training', shuffle=True\n",
        ")\n",
        "\n",
        "val_data = train_gen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', subset='validation', shuffle=False\n",
        ")\n",
        "\n",
        "# Indexing the classes\n",
        "num_classes = len(train_data.class_indices)\n",
        "class_indices = train_data.class_indices\n",
        "index_to_class = {v: k for k, v in class_indices.items()}\n"
      ],
      "metadata": {
        "id": "ihPRDKwn8vbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining, compiling and training the model"
      ],
      "metadata": {
        "id": "G-ZWy7Sv9JVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose MobileNetV2 because it gave us more promising accuracy with less time-cost and showed more potential. The experiment for testing the models are available in Github in form of code and report.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We Also saved the best parameters for future refrences and entered a manual F1 Score callback."
      ],
      "metadata": {
        "id": "56ts87pj9LmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(224, 224, 3)),\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "class F1ScoreCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        for x_batch, y_batch in val_data:\n",
        "            preds = model.predict(x_batch, verbose=0)\n",
        "            val_preds.extend(np.argmax(preds, axis=1))\n",
        "            val_labels.extend(np.argmax(y_batch, axis=1))\n",
        "            if len(val_labels) >= val_data.samples:\n",
        "                break\n",
        "        f1 = f1_score(val_labels, val_preds, average='micro')\n",
        "        print(f\"\\nðŸŽ¯ Validation Micro F1 Score: {f1:.4f}\")\n",
        "\n",
        "callbacks = [\n",
        "    F1ScoreCallback(),\n",
        "    EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(MODEL_PATH, save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "7hPRvS9k9fpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the reporting"
      ],
      "metadata": {
        "id": "swP1WZNk-MUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading, Extracting and cleaning the test data"
      ],
      "metadata": {
        "id": "qb_UKuIV-Qsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(TEST_DIR):\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={TEST_ZIP_ID}\", TEST_ZIP_PATH, quiet=False)\n",
        "    with zipfile.ZipFile(TEST_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"data\")\n",
        "\n",
        "clean_directory(TEST_DIR)\n"
      ],
      "metadata": {
        "id": "GilzUvDs-ZfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting the classes of test data"
      ],
      "metadata": {
        "id": "cl2PJaZ4-dKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”® Predict on Test Images\n",
        "predictions = []\n",
        "filenames = []\n",
        "corrupt_files = []\n",
        "\n",
        "model = keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "for fname in sorted(os.listdir(TEST_DIR)):\n",
        "    if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        continue\n",
        "    try:\n",
        "        path = os.path.join(TEST_DIR, fname)\n",
        "        img = Image.open(path).convert(\"RGB\").resize(IMG_SIZE)\n",
        "        arr = preprocess_input(img_to_array(img))\n",
        "        arr = np.expand_dims(arr, axis=0)\n",
        "\n",
        "        pred = model.predict(arr, verbose=0)\n",
        "        pred_class = index_to_class[np.argmax(pred[0])]\n",
        "\n",
        "        filenames.append(fname)\n",
        "        predictions.append(pred_class)\n",
        "\n",
        "    except (UnidentifiedImageError, OSError, ValueError):\n",
        "        corrupt_files.append(fname)\n"
      ],
      "metadata": {
        "id": "UBSyIEcY-iSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reporting the test results"
      ],
      "metadata": {
        "id": "Kr7BWUiN-p0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We anticipated that we might have some \"bad\" data similar to training data. This is where we would report them."
      ],
      "metadata": {
        "id": "6js2H-In-xyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOS_nRPveKTE",
        "outputId": "a4751ec0-9547-49f7-9d97-28356623f7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15CHt2ueS4c7emHpmzFHC3c0TGd51Mnvz\n",
            "From (redirected): https://drive.google.com/uc?id=15CHt2ueS4c7emHpmzFHC3c0TGd51Mnvz&confirm=t&uuid=ce7851b5-4a28-4ca1-a929-84729564cdb0\n",
            "To: /content/train_data.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898M/898M [00:16<00:00, 54.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19276 images belonging to 22 classes.\n",
            "Found 4808 images belonging to 22 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849ms/step - accuracy: 0.4381 - loss: 2.0163"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.6219\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 1s/step - accuracy: 0.4387 - loss: 2.0143 - val_accuracy: 0.6188 - val_loss: 1.5284\n",
            "Epoch 2/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817ms/step - accuracy: 0.7824 - loss: 0.7392"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.7107\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 1s/step - accuracy: 0.7824 - loss: 0.7391 - val_accuracy: 0.7157 - val_loss: 1.0685\n",
            "Epoch 3/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811ms/step - accuracy: 0.8314 - loss: 0.5652"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.7627\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 1s/step - accuracy: 0.8314 - loss: 0.5651 - val_accuracy: 0.7619 - val_loss: 0.9141\n",
            "Epoch 4/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816ms/step - accuracy: 0.8598 - loss: 0.4523"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.7964\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 1s/step - accuracy: 0.8598 - loss: 0.4523 - val_accuracy: 0.8012 - val_loss: 0.7630\n",
            "Epoch 5/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811ms/step - accuracy: 0.8883 - loss: 0.3761\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.7885\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 1s/step - accuracy: 0.8883 - loss: 0.3761 - val_accuracy: 0.7949 - val_loss: 0.7770\n",
            "Epoch 6/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812ms/step - accuracy: 0.9030 - loss: 0.3102"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.8240\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 1s/step - accuracy: 0.9030 - loss: 0.3102 - val_accuracy: 0.8228 - val_loss: 0.6562\n",
            "Epoch 7/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813ms/step - accuracy: 0.9181 - loss: 0.2603"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.8386\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 1s/step - accuracy: 0.9181 - loss: 0.2604 - val_accuracy: 0.8409 - val_loss: 0.5949\n",
            "Epoch 8/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808ms/step - accuracy: 0.9290 - loss: 0.2295\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.8209\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 1s/step - accuracy: 0.9289 - loss: 0.2295 - val_accuracy: 0.8251 - val_loss: 0.7047\n",
            "Epoch 9/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805ms/step - accuracy: 0.9369 - loss: 0.2011\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.8380\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.9369 - loss: 0.2011 - val_accuracy: 0.8392 - val_loss: 0.6419\n",
            "Epoch 10/15\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819ms/step - accuracy: 0.9439 - loss: 0.1788\n",
            "ðŸŽ¯ Validation Micro F1 Score: 0.8415\n",
            "\u001b[1m302/302\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 1s/step - accuracy: 0.9439 - loss: 0.1788 - val_accuracy: 0.8469 - val_loss: 0.6095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Predictions saved to submission.csv\n",
            "âœ… No corrupt test images found.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "submission = pd.DataFrame({\"name\": filenames, \"predicted\": predictions})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"âœ… Predictions saved to submission.csv\")\n",
        "\n",
        "if corrupt_files:\n",
        "    print(f\"âš ï¸ Skipped {len(corrupt_files)} corrupt images:\")\n",
        "    for c in corrupt_files:\n",
        "        print(\" -\", c)\n",
        "else:\n",
        "    print(\"âœ… No corrupt test images found.\")\n"
      ]
    }
  ]
}